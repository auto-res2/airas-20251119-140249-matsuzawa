run_id: proposed-iter1-Qwen3-0.6B-gsm8k
method: proposed
algorithm:
  name: OMEGA-LR
  params:
    rho_0: 1.0e-3
    beta_g: 0.5
    kappa: 0.3
    beta_w: 0.99
    beta_rho: 0.95
    eps: 1.0e-8
    lr_clip_factor:
      min: 0.2
      max: 5.0
validation:
  buffer_size: 256
  k: 4
model:
  name: Qwen/Qwen2.5-0.5B
  num_layers: 24
  hidden_size: 896
  num_attention_heads: 14
  precision: fp16
dataset:
  name: gsm8k
  config: main
  train_split: train
  val_split: test
  max_seq_length: 512
  batch_size: 8
  gradient_accumulation_steps: 8
training:
  epochs: 3
  optimizer:
    name: adamw
    base_lr: 1.0e-5
    weight_decay: 0.1
    betas: [0.9, 0.999]
  scheduler: constant
  seed_list: [42, 43, 44]
logging:
  metrics:
    - em
    - steps_to_55_em
    - layer_update_ratio
    - gpu_overhead
compute:
  device: cuda
  precision: fp16
optuna:
  n_trials: 20
  direction: maximize
  metric: em
  search_space:
    algorithm.params.rho_0:
      type: loguniform
      low: 5e-4
      high: 2e-3
    algorithm.params.beta_g:
      type: uniform
      low: 0.3
      high: 0.7
    algorithm.params.kappa:
      type: uniform
      low: 0.2
      high: 0.5
    training.optimizer.base_lr:
      type: loguniform
      low: 5e-6
      high: 2e-5
callbacks:
  checkpoint:
    monitor: em
    mode: max
  early_stopping:
    monitor: em
    patience: 2000
